{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayush/Desktop/ayush/UvA-multi-modal-ai/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ayush/Desktop/ayush/UvA-multi-modal-ai/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, BertModel, Wav2Vec2Model\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "class AdaptiveModalityEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=768):\n",
    "        super(AdaptiveModalityEncoder, self).__init__()\n",
    "        self.transformer_block = nn.TransformerEncoderLayer(d_model=input_dim, nhead=8)\n",
    "        self.gate = nn.Linear(input_dim, 1)  # Gating mechanism to assign relevance to each modality\n",
    "    \n",
    "    def forward(self, modality_features):\n",
    "        # modality_features is a list of tensors (img_features, text_features, audio_features)\n",
    "        stacked_features = torch.stack(modality_features, dim=1)\n",
    "        transformed = self.transformer_block(stacked_features)  # (batch, modalities, dim)\n",
    "        gated_weights = torch.softmax(self.gate(transformed).squeeze(-1), dim=1)\n",
    "        weighted_features = (gated_weights.unsqueeze(-1) * transformed).sum(dim=1)\n",
    "        return weighted_features, gated_weights\n",
    "\n",
    "class HierarchicalCrossModalityAttention(nn.Module):\n",
    "    def __init__(self, input_dim=768):\n",
    "        super(HierarchicalCrossModalityAttention, self).__init__()\n",
    "        self.cross_attention1 = nn.MultiheadAttention(embed_dim=input_dim, num_heads=8)\n",
    "        self.cross_attention2 = nn.MultiheadAttention(embed_dim=input_dim, num_heads=8)\n",
    "    \n",
    "    def forward(self, img_features, text_features, audio_features):\n",
    "        # First level of cross-modality attention with text as the pivot modality\n",
    "        text_img_attn, _ = self.cross_attention1(text_features, img_features, img_features)\n",
    "        text_audio_attn, _ = self.cross_attention1(text_features, audio_features, audio_features)\n",
    "        fused_text = (text_img_attn + text_audio_attn) / 2\n",
    "        \n",
    "        # Second level of cross-modality attention with image as the pivot modality\n",
    "        img_text_attn, _ = self.cross_attention2(img_features, fused_text, fused_text)\n",
    "        img_audio_attn, _ = self.cross_attention2(img_features, audio_features, audio_features)\n",
    "        fused_img = (img_text_attn + img_audio_attn) / 2\n",
    "        \n",
    "        # Final fusion\n",
    "        fused_features = (fused_text + fused_img + audio_features) / 3\n",
    "        return fused_features\n",
    "\n",
    "class MACAST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MACAST, self).__init__()\n",
    "        \n",
    "        # Modality encoders\n",
    "        self.image_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        self.text_encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.audio_encoder = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        \n",
    "        # Adaptive Modality Encoder\n",
    "        self.adaptive_modality_encoder = AdaptiveModalityEncoder()\n",
    "        \n",
    "        # Hierarchical Cross-Modality Attention\n",
    "        self.hierarchical_attention = HierarchicalCrossModalityAttention()\n",
    "        \n",
    "        # Sentiment classification head\n",
    "        self.classification_head = nn.Linear(768, 3)  # 3 sentiment classes\n",
    "        \n",
    "        # Auxiliary losses for self-supervised modality contrastive alignment\n",
    "        self.contrastive_temp = 0.07\n",
    "        self.contrastive_loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, image, text, audio):\n",
    "        # Encode each modality\n",
    "        img_features = self.image_encoder(pixel_values=image).last_hidden_state[:, 0]\n",
    "        text_features = self.text_encoder(input_ids=text[\"input_ids\"], attention_mask=text[\"attention_mask\"]).last_hidden_state[:, 0]\n",
    "        audio_features = self.audio_encoder(input_values=audio[\"input_values\"]).last_hidden_state[:, 0]\n",
    "        \n",
    "        # Adaptive Modality Encoding\n",
    "        weighted_features, gated_weights = self.adaptive_modality_encoder([img_features, text_features, audio_features])\n",
    "        \n",
    "        # Hierarchical Cross-Modality Attention Fusion\n",
    "        fused_features = self.hierarchical_attention(img_features, text_features, audio_features)\n",
    "        \n",
    "        # Classification head\n",
    "        logits = self.classification_head(fused_features)\n",
    "        \n",
    "        # Contrastive alignment for modality embeddings\n",
    "        img_proj, text_proj, audio_proj = map(normalize, [img_features, text_features, audio_features])\n",
    "        contrastive_loss = self.modality_contrastive_loss(img_proj, text_proj, audio_proj)\n",
    "        \n",
    "        return logits, contrastive_loss\n",
    "\n",
    "    def modality_contrastive_loss(self, img_proj, text_proj, audio_proj):\n",
    "        # Implementing contrastive loss for cross-modal alignment\n",
    "        batch_size = img_proj.size(0)\n",
    "        labels = torch.arange(batch_size).to(img_proj.device)\n",
    "        \n",
    "        logits_img_text = torch.mm(img_proj, text_proj.T) / self.contrastive_temp\n",
    "        logits_img_audio = torch.mm(img_proj, audio_proj.T) / self.contrastive_temp\n",
    "        logits_text_audio = torch.mm(text_proj, audio_proj.T) / self.contrastive_temp\n",
    "        \n",
    "        loss_img_text = self.contrastive_loss_fn(logits_img_text, labels)\n",
    "        loss_img_audio = self.contrastive_loss_fn(logits_img_audio, labels)\n",
    "        loss_text_audio = self.contrastive_loss_fn(logits_text_audio, labels)\n",
    "        \n",
    "        return (loss_img_text + loss_img_audio + loss_text_audio) / 3\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, texts, audios, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, contrastive_loss = model(images, texts, audios)\n",
    "            cls_loss = criterion(logits, labels)\n",
    "            total_loss = cls_loss + contrastive_loss  # Joint loss\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
